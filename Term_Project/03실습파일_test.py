# 03 Ïã§Ïäµ ÌååÏùº_test.py
# Ï£ºÌîºÌÑ∞ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú Î≥ÄÌôòÎê®. ÎßàÌÅ¨Îã§Ïö¥ ÏÖÄÏùÄ Ï£ºÏÑù Ï≤òÎ¶¨Îê®.

# ### Ïã§Ïäµ Ï§ÄÎπÑ
# - Îç∞Ïù¥ÌÑ∞ÏÖã ÏÑ§Ïπò

# %pip install torchvision pycocotools pillow gdown  # Ï£ºÌîºÌÑ∞ Îß§ÏßÅ Î™ÖÎ†πÏñ¥Îäî Ï£ºÏÑù Ï≤òÎ¶¨
# Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑúÎäî ÏïÑÎûò Î™ÖÎ†πÏñ¥Î•º ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú ÏßÅÏ†ë Ïã§ÌñâÌïòÏÑ∏Ïöî.
# pip install torchvision pycocotools pillow gdown

import os
import zipfile
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models.detection import rpn, roi_heads
from torchvision.models.detection.transform import GeneralizedRCNNTransform
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.ops import MultiScaleRoIAlign, boxes as box_ops
from torchvision.ops.feature_pyramid_network import LastLevelP6P7
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from tqdm import tqdm
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
import numpy as np
from collections import OrderedDict

# CUDA ÏÑ§Ï†ï
torch.cuda.empty_cache()
torch.backends.cudnn.benchmark = True

# Google DriveÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú (gdown ÏÇ¨Ïö©)
# gdrive_id = "1-tPJHwYIdmnQsk_o_tcynZIH86cfcFzn"
# os.system(f"gdown --id {gdrive_id} -O SFU.zip")

# with zipfile.ZipFile("./SFU.zip", 'r') as zip_ref:
#     zip_ref.extractall("./SFU")

# ### Î™®Îç∏
# - Î™®Îç∏ Î∂ÄÎ∂ÑÏùÑ ÏûëÏÑ±.
# - ./model_checkpoint Ìè¥ÎçîÎ•º ÎßåÎì§Í≥†, ÌÖåÏä§Ìä∏ÌïòÎ†§Îäî Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÎÑ£ÏúºÏÑ∏Ïöî.

# üéØ Train.pyÏôÄ ÏôÑÏ†ÑÌûà ÎèôÏùºÌïú ÌÅ¥ÎûòÏä§ ÏÑ§Ï†ï
COCO_TO_SFU_MAPPING = {
    1: 1,   # person
    2: 2,   # bicycle
    3: 3,   # car
    4: 4,   # motorcycle
    6: 5,   # bus
    8: 6,   # truck
}
SFU_CLASS_IDS = set(COCO_TO_SFU_MAPPING.keys())  # {1, 2, 3, 4, 6, 8}
GOOD_CLASSES = [1, 2, 3, 4, 6, 8]  # SFU ÏõêÎ≥∏ ÌÅ¥ÎûòÏä§
CLASS_NAMES = {1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 6: "bus", 8: "truck"}

print(f"üéØ Train.pyÏôÄ ÎèôÏùºÌïú ÌÅ¥ÎûòÏä§ ÏÑ§Ï†ï:")
print(f"   - SFU_CLASS_IDS: {SFU_CLASS_IDS}")
print(f"   - GOOD_CLASSES: {GOOD_CLASSES}")
print(f"   - ÌÅ¥ÎûòÏä§ Ïù¥Î¶Ñ: {CLASS_NAMES}")

# ===== Train.pyÏôÄ Ï†ïÌôïÌûà ÏùºÏπòÌïòÎäî Î™®Îç∏ ÌÅ¥ÎûòÏä§ =====
class MyNetwork(nn.Module):
    """Train.pyÏôÄ Ï†ïÌôïÌûà ÎèôÏùºÌïú Íµ¨Ï°∞"""
    def __init__(self, num_classes=7, backbone_name='resnet50', 
                 min_size=800, max_size=1333, 
                 box_detections_per_img=300,  # üîß 100 ‚Üí 300 (Îçî ÎßéÏùÄ Í≤ÄÏ∂ú)
                 box_score_thresh=0.01,       # üîß 0.05 ‚Üí 0.01 (Îçî ÎÇÆÏùÄ ÏûÑÍ≥ÑÍ∞í)
                 box_nms_thresh=0.3):         # üîß 0.5 ‚Üí 0.3 (Îçî Í∞ïÌïú NMS)
        super(MyNetwork, self).__init__()
        
        self.num_classes = num_classes
        
        # Backbone: ResNet50 with FPN
        extra_blocks = LastLevelP6P7(256, 256)
        self.backbone = resnet_fpn_backbone(
            backbone_name, 
            pretrained=True,
            trainable_layers=5,
            returned_layers=[1, 2, 3, 4],
            extra_blocks=extra_blocks
        )
        
        out_channels = self.backbone.out_channels
        
        # RPN ÏÑ§Ï†ï
        anchor_sizes = ((16,), (32,), (64,), (128,), (256,), (512,))
        aspect_ratios = ((0.5, 1.0, 2.0),) * 6
        
        rpn_anchor_generator = rpn.AnchorGenerator(anchor_sizes, aspect_ratios)
        rpn_head = rpn.RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])
        
        # RPN parameters
        rpn_pre_nms_top_n_train = 2000
        rpn_pre_nms_top_n_test = 1000
        rpn_post_nms_top_n_train = 2000
        rpn_post_nms_top_n_test = 1000
        rpn_nms_thresh = 0.7
        rpn_fg_iou_thresh = 0.7
        rpn_bg_iou_thresh = 0.3
        rpn_batch_size_per_image = 256
        rpn_positive_fraction = 0.5
        
        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)
        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)
        
        self.rpn = rpn.RegionProposalNetwork(
            rpn_anchor_generator, rpn_head,
            rpn_fg_iou_thresh, rpn_bg_iou_thresh,
            rpn_batch_size_per_image, rpn_positive_fraction,
            rpn_pre_nms_top_n, rpn_post_nms_top_n,
            rpn_nms_thresh
        )
        
        # ROI Pooling
        box_roi_pool = MultiScaleRoIAlign(
            featmap_names=['0', '1', '2', '3'],
            output_size=7,
            sampling_ratio=2
        )
        
        # Faster R-CNN Head
        resolution = box_roi_pool.output_size[0]
        representation_size = 2048
        
        box_head = TwoMLPHead(
            out_channels * resolution ** 2,
            representation_size,
            dropout_p=0.5
        )
        
        box_predictor = FastRCNNPredictor(representation_size, num_classes)
        
        # ROI Head parameters
        box_fg_iou_thresh = 0.5
        box_bg_iou_thresh = 0.5
        box_batch_size_per_image = 512
        box_positive_fraction = 0.25
        bbox_reg_weights = None
        
        self.roi_heads = roi_heads.RoIHeads(
            box_roi_pool, box_head, box_predictor,
            box_fg_iou_thresh, box_bg_iou_thresh,
            box_batch_size_per_image, box_positive_fraction,
            bbox_reg_weights,
            box_score_thresh, box_nms_thresh, box_detections_per_img
        )
        
        # Transform
        self.transform = GeneralizedRCNNTransform(
            min_size, max_size,
            image_mean=[0.485, 0.456, 0.406],
            image_std=[0.229, 0.224, 0.225]
        )
        
    def forward(self, images, targets=None):
        if self.training and targets is None:
            raise ValueError("In training mode, targets should be provided")
        
        original_image_sizes = []
        for img in images:
            val = img.shape[-2:]
            original_image_sizes.append((val[0], val[1]))
        
        images, targets = self.transform(images, targets)
        features = self.backbone(images.tensors)
        
        if isinstance(features, torch.Tensor):
            features = OrderedDict([('0', features)])
        
        proposals, proposal_losses = self.rpn(images, features, targets)
        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)
        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)
        
        losses = {}
        losses.update(detector_losses)
        losses.update(proposal_losses)
        
        if self.training:
            return losses
        
        return detections


class TwoMLPHead(nn.Module):
    def __init__(self, in_channels, representation_size, dropout_p=0.5):
        super(TwoMLPHead, self).__init__()
        self.fc6 = nn.Linear(in_channels, representation_size)
        self.fc7 = nn.Linear(representation_size, representation_size)
        self.dropout = nn.Dropout(p=dropout_p)
        
    def forward(self, x):
        x = x.flatten(start_dim=1)
        x = F.relu(self.fc6(x))
        x = self.dropout(x)
        x = F.relu(self.fc7(x))
        x = self.dropout(x)
        return x


class FastRCNNPredictor(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(FastRCNNPredictor, self).__init__()
        self.cls_score = nn.Linear(in_channels, num_classes)
        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)
        
    def forward(self, x):
        if x.dim() == 4:
            x = x.flatten(start_dim=1)
        scores = self.cls_score(x)
        bbox_deltas = self.bbox_pred(x)
        return scores, bbox_deltas


# ===== SFU Dataset ÌÅ¥ÎûòÏä§ =====
class SFUDatasetFixed(Dataset):
    def __init__(self, root_dir, dataset_name, train_cat2label, transform=None):
        self.root_dir = root_dir
        self.dataset_name = dataset_name
        self.transform = transform
        
        dataset_path = os.path.join(root_dir, dataset_name)
        self.ann_file = os.path.join(dataset_path, f"annotations/{dataset_name}.json")
        self.img_dir = os.path.join(dataset_path, "images")
        
        self.coco = COCO(self.ann_file)
        self.img_ids = self.coco.getImgIds()
        
        self.cat2label = train_cat2label
        self.label2cat = {v: k for k, v in self.cat2label.items()}
        
        print(f"üîß {dataset_name} ÎèÑÎ©îÏù∏ Îß§Ìïë:")
        print(f"   - Ïù¥ÎØ∏ÏßÄ Ïàò: {len(self.img_ids)}")
        print(f"   - Train cat2label: {self.cat2label}")
        
        self.base_transform = T.Compose([T.ToTensor()])
        
    def __len__(self):
        return len(self.img_ids)
    
    def __getitem__(self, index):
        img_id = self.img_ids[index]
        img_info = self.coco.loadImgs(img_id)[0]
        img_path = os.path.join(self.img_dir, img_info["file_name"])
        
        with open(img_path, "rb") as f:
            img = Image.open(f).convert("RGB")
        
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        anns = self.coco.loadAnns(ann_ids)
        
        bboxes = []
        labels = []
        areas = []
        iscrowd = []
        
        for ann in anns:
            sfu_class_id = ann["category_id"]
            
            if sfu_class_id not in self.cat2label:
                continue
                
            x, y, w, h = ann["bbox"]
            if w > 0 and h > 0:
                bbox = [x, y, x + w, y + h]
                bboxes.append(bbox)
                
                internal_label = self.cat2label[sfu_class_id]
                labels.append(internal_label)
                
                areas.append(ann["area"])
                iscrowd.append(ann.get("iscrowd", 0))
        
        if len(bboxes) == 0:
            bboxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
            areas = torch.zeros((0,), dtype=torch.float32)
            iscrowd = torch.zeros((0,), dtype=torch.int64)
        else:
            bboxes = torch.as_tensor(bboxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
            areas = torch.as_tensor(areas, dtype=torch.float32)
            iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)
        
        target = {
            "boxes": bboxes,
            "labels": labels,
            "image_id": torch.tensor([img_id]),
            "area": areas,
            "iscrowd": iscrowd
        }
        
        img = self.base_transform(img)
        
        if self.transform:
            img, target = self.transform(img, target)
        
        return img, target


def collate_fn(batch):
    return tuple(zip(*batch))


# üöÄ ÎåÄÌè≠ Í∞úÏÑ†Îêú ÎèÑÎ©îÏù∏ ÏÑ§Ï†ï
def get_domain_settings(dataset_name):
    """Îç∞Ïù¥ÌÑ∞ÏÖãÎ≥Ñ ÏµúÏ†ÅÌôîÎêú Í≥µÍ≤©Ï†Å ÏÑ§Ï†ï"""
    dataset_lower = dataset_name.lower()
    
    if 'partyscene_832x480_50' in dataset_lower:
        return {
            'conf_thresh': 0.03,      # 0.12 ‚Üí 0.03
            'nms_thresh': 0.4,        # 0.65 ‚Üí 0.4
            'person_boost': 3.0,      # 1.4 ‚Üí 3.0 (ÌååÌã∞ = ÏÇ¨Îûå Ï§ëÏã¨)
            'vehicle_boost': 1.8,     # 1.2 ‚Üí 1.8
            'animal_boost': 1.8,      # 1.3 ‚Üí 1.8
            'min_area': 50,           # ÏÉàÎ°ú Ï∂îÍ∞Ä
            'max_detections': 200     # ÏÉàÎ°ú Ï∂îÍ∞Ä
        }
    elif 'basketballpass_416x240_50' in dataset_lower:
        return {
            'conf_thresh': 0.01,      # 0.13 ‚Üí 0.01 (Îß§Ïö∞ Í≥µÍ≤©Ï†Å)
            'nms_thresh': 0.25,       # 0.62 ‚Üí 0.25
            'person_boost': 4.0,      # 1.3 ‚Üí 4.0 (ÎÜçÍµ¨ = ÏÇ¨Îûå Ï§ëÏã¨)
            'vehicle_boost': 1.5,
            'animal_boost': 1.5,
            'min_area': 30,           # ÏûëÏùÄ Ìï¥ÏÉÅÎèÑÏö©
            'max_detections': 150
        }
    elif 'racehorses' in dataset_lower:
        return {
            'conf_thresh': 0.005,     # 0.10 ‚Üí 0.005 (Í∑πÎèÑÎ°ú Í≥µÍ≤©Ï†Å)
            'nms_thresh': 0.25,       # 0.67 ‚Üí 0.25
            'person_boost': 2.0,
            'vehicle_boost': 1.2,
            'animal_boost': 6.0,      # 1.6 ‚Üí 6.0 (Îßê Í∑πÎåÄÌôî)
            'animal_mode': True,
            'min_area': 100,
            'max_detections': 100
        }
    elif 'bqsquare_416x240_60' in dataset_lower:
        return {
            'conf_thresh': 0.08,      # 0.23 ‚Üí 0.08 (ÏÑ±Í≥µ ÏºÄÏù¥Ïä§ ÏïΩÍ∞Ñ ÏôÑÌôî)
            'nms_thresh': 0.45,       # 0.52 ‚Üí 0.45
            'person_boost': 2.0,      # 1.2 ‚Üí 2.0
            'vehicle_boost': 2.0,     # 1.25 ‚Üí 2.0
            'animal_boost': 1.8,      # 1.2 ‚Üí 1.8
            'min_area': 40,
            'max_detections': 150
        }
    elif 'bqmall' in dataset_lower or 'bqterrace' in dataset_lower:
        return {
            'conf_thresh': 0.05,      # 0.20 ‚Üí 0.05
            'nms_thresh': 0.4,        # 0.52 ‚Üí 0.4
            'person_boost': 2.5,      # 1.2 ‚Üí 2.5 (ÏáºÌïëÎ™∞/ÌÖåÎùºÏä§ = ÏÇ¨Îûå)
            'vehicle_boost': 2.0,     # 1.2 ‚Üí 2.0
            'animal_boost': 1.8,      # 1.2 ‚Üí 1.8
            'min_area': 60,
            'max_detections': 180
        }
    elif 'basketball' in dataset_lower:
        return {
            'conf_thresh': 0.01,      # 0.08 ‚Üí 0.01
            'nms_thresh': 0.25,       # 0.62 ‚Üí 0.25
            'person_boost': 4.0,      # 1.4 ‚Üí 4.0
            'vehicle_boost': 1.5,
            'animal_boost': 1.5,
            'min_area': 40,
            'max_detections': 150
        }
    elif 'traffic' in dataset_lower:
        return {
            'conf_thresh': 0.05,      # 0.18 ‚Üí 0.05
            'nms_thresh': 0.35,       # 0.52 ‚Üí 0.35
            'person_boost': 2.0,      # 1.2 ‚Üí 2.0
            'vehicle_boost': 3.0,     # 1.3 ‚Üí 3.0 (ÍµêÌÜµ = Ï∞®Îüâ Ï§ëÏã¨)
            'animal_boost': 1.8,      # 1.2 ‚Üí 1.8
            'min_area': 80,
            'max_detections': 200
        }
    elif 'park' in dataset_lower:
        return {
            'conf_thresh': 0.03,      # 0.12 ‚Üí 0.03
            'nms_thresh': 0.35,       # 0.62 ‚Üí 0.35
            'person_boost': 3.0,      # 1.4 ‚Üí 3.0 (Í≥µÏõê = ÏÇ¨Îûå)
            'vehicle_boost': 2.0,     # 1.2 ‚Üí 2.0
            'animal_boost': 2.5,      # 1.3 ‚Üí 2.5
            'min_area': 70,
            'max_detections': 180
        }
    else:
        # Í∏∞Î≥∏ Í≥µÍ≤©Ï†Å ÏÑ§Ï†ï
        return {
            'conf_thresh': 0.02,      # 0.08 ‚Üí 0.02
            'nms_thresh': 0.35,       # 0.65 ‚Üí 0.35
            'person_boost': 2.5,      # 1.3 ‚Üí 2.5
            'vehicle_boost': 2.0,     # 1.2 ‚Üí 2.0
            'animal_boost': 2.0,      # 1.3 ‚Üí 2.0
            'min_area': 60,
            'max_detections': 150
        }


def apply_smart_boost(scores, labels, settings, label2cat):
    """ÎåÄÌè≠ Í∞ïÌôîÎêú Ïä§ÎßàÌä∏ ÌÅ¥ÎûòÏä§ Î∂ÄÏä§Ìä∏"""
    boosted_scores = scores.clone()
    
    for i, internal_label in enumerate(labels):
        sfu_class_id = label2cat.get(internal_label.item(), None)
        if sfu_class_id is None:
            continue
            
        # Person Î∂ÄÏä§Ìä∏ (SFU ÌÅ¥ÎûòÏä§ 1)
        if sfu_class_id == 1 and 'person_boost' in settings:
            boosted_scores[i] *= settings['person_boost']
        
        # Vehicle Î∂ÄÏä§Ìä∏ (car=3, bus=6, truck=8)
        elif sfu_class_id in [3, 6, 8] and 'vehicle_boost' in settings:
            boosted_scores[i] *= settings['vehicle_boost']
        
        # Animal/Transport Î∂ÄÏä§Ìä∏ (bicycle=2, motorcycle=4)
        elif sfu_class_id in [2, 4] and 'animal_boost' in settings:
            boosted_scores[i] *= settings['animal_boost']
            
        # ÌäπÏàò ÎèôÎ¨º Î™®Îìú
        if settings.get('animal_mode', False) and sfu_class_id in [2, 4]:
            boosted_scores[i] *= 2.0  # Ï∂îÍ∞Ä Í∑πÎåÄÌôî
    
    return boosted_scores


def apply_advanced_nms(boxes, scores, labels, settings):
    """Í≥†Í∏â NMS Ï≤òÎ¶¨"""
    if len(boxes) == 0:
        return boxes, scores, labels
    
    # 1. Î∞ïÏä§ ÌÅ¨Í∏∞ ÌïÑÌÑ∞ÎßÅ
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    min_area = settings.get('min_area', 50)
    area_mask = areas >= min_area
    
    if area_mask.sum() == 0:
        return torch.zeros((0, 4)), torch.zeros(0), torch.zeros(0)
    
    boxes = boxes[area_mask]
    scores = scores[area_mask]
    labels = labels[area_mask]
    
    # 2. ÌÅ¥ÎûòÏä§Î≥Ñ NMS
    keep_indices = []
    nms_thresh = settings['nms_thresh']
    
    for class_id in torch.unique(labels):
        class_mask = labels == class_id
        if class_mask.sum() == 0:
            continue
            
        class_boxes = boxes[class_mask]
        class_scores = scores[class_mask]
        
        # ÌÅ¥ÎûòÏä§Î≥Ñ NMS Ï†ÅÏö©
        keep = box_ops.nms(class_boxes, class_scores, nms_thresh)
        
        # ÏõêÎ≥∏ Ïù∏Îç±Ïä§Î°ú Î≥ÄÌôò
        original_indices = torch.where(class_mask)[0]
        keep_indices.extend(original_indices[keep].tolist())
    
    if len(keep_indices) == 0:
        return torch.zeros((0, 4)), torch.zeros(0), torch.zeros(0)
    
    keep_indices = torch.tensor(keep_indices, dtype=torch.long)
    
    # 3. ÏµúÎåÄ Í≤ÄÏ∂ú Ïàò Ï†úÌïú
    max_detections = settings.get('max_detections', 150)
    if len(keep_indices) > max_detections:
        # Ï†êÏàò Í∏∞Ï§Ä ÏÉÅÏúÑ NÍ∞úÎßå ÏÑ†ÌÉù
        selected_scores = scores[keep_indices]
        _, top_indices = torch.topk(selected_scores, max_detections)
        keep_indices = keep_indices[top_indices]
    
    return boxes[keep_indices], scores[keep_indices], labels[keep_indices]


# ===== üöÄ ÎåÄÌè≠ Í∞úÏÑ†Îêú ÌèâÍ∞Ä Ìï®Ïàò =====
@torch.no_grad()
def evaluate_mAP_optimized(model, dataloader, device, label2cat, dataset_name=""):
    """ÏµúÏ†ÅÌôîÎêú mAP ÌèâÍ∞Ä Ìï®Ïàò"""
    model.eval()
    coco_gt = dataloader.dataset.coco
    results = []
    
    settings = get_domain_settings(dataset_name)
    conf_thresh = settings['conf_thresh']
    
    print(f"üöÄ ÏµúÏ†ÅÌôîÎêú {dataset_name} ÌèâÍ∞Ä")
    print(f"   - Ïã†Î¢∞ÎèÑ ÏûÑÍ≥ÑÍ∞í: {conf_thresh}")
    print(f"   - NMS ÏûÑÍ≥ÑÍ∞í: {settings['nms_thresh']}")
    print(f"   - ÏµúÏÜå Î©¥Ï†Å: {settings.get('min_area', 50)}")
    print(f"   - ÏµúÎåÄ Í≤ÄÏ∂ú: {settings.get('max_detections', 150)}")
    print(f"   - Î∂ÄÏä§Ìä∏ ÏÑ§Ï†ï: person={settings.get('person_boost', 1.0)}, "
          f"vehicle={settings.get('vehicle_boost', 1.0)}, "
          f"animal={settings.get('animal_boost', 1.0)}")
    
    total_detections = 0
    total_after_conf = 0
    total_after_nms = 0
    total_final = 0
    
    for images, targets in tqdm(dataloader, desc="Optimized Evaluating"):
        images = list(img.to(device) for img in images)
        outputs = model(images)
        
        for idx, output in enumerate(outputs):
            image_id = targets[idx]["image_id"].item() if isinstance(targets[idx]["image_id"], torch.Tensor) else targets[idx]["image_id"]
            
            boxes = output["boxes"].cpu()
            scores = output["scores"].cpu()
            labels = output["labels"].cpu()
            
            total_detections += len(boxes)
            
            if len(boxes) == 0:
                continue
            
            # 1. Ïä§ÎßàÌä∏ ÌÅ¥ÎûòÏä§ Î∂ÄÏä§Ìä∏ Ï†ÅÏö©
            boosted_scores = apply_smart_boost(scores, labels, settings, label2cat)
            
            # 2. Ïã†Î¢∞ÎèÑ ÌïÑÌÑ∞ÎßÅ
            conf_mask = boosted_scores >= conf_thresh
            if conf_mask.sum() == 0:
                continue
                
            boxes = boxes[conf_mask]
            scores = boosted_scores[conf_mask]
            labels = labels[conf_mask]
            total_after_conf += len(boxes)
            
            # 3. Í≥†Í∏â NMS Ï†ÅÏö©
            boxes, scores, labels = apply_advanced_nms(boxes, scores, labels, settings)
            total_after_nms += len(boxes)
            
            if len(boxes) == 0:
                continue
            
            # 4. Í≤∞Í≥º Î≥ÄÌôò
            for box, score, internal_label in zip(boxes, scores, labels):
                sfu_class_id = label2cat.get(internal_label.item(), None)
                if sfu_class_id is None:
                    continue
                
                x1, y1, x2, y2 = box.tolist()
                x, y = x1, y1
                w, h = x2 - x1, y2 - y1
                
                if w <= 1 or h <= 1:
                    continue
                
                result = {
                    "image_id": int(image_id),
                    "category_id": int(sfu_class_id),
                    "bbox": [float(x), float(y), float(w), float(h)],
                    "score": float(score),
                }
                results.append(result)
                total_final += 1
    
    print(f"üìä ÏµúÏ†ÅÌôîÎêú Í≤ÄÏ∂ú ÌÜµÍ≥Ñ:")
    print(f"  - ÏõêÎ≥∏ Í≤ÄÏ∂ú: {total_detections}")
    print(f"  - Ïã†Î¢∞ÎèÑ ÌõÑ: {total_after_conf}")
    print(f"  - NMS ÌõÑ: {total_after_nms}")
    print(f"  - ÏµúÏ¢Ö Í≤ÄÏ∂ú: {total_final}")
    
    if len(results) == 0:
        print("‚ùå Í≤ÄÏ∂úÎêú Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§!")
        return 0.0
    
    # ÌÅ¥ÎûòÏä§Î≥Ñ Í≤ÄÏ∂ú Ïàò ÌôïÏù∏
    class_counts = {}
    for result in results:
        cls_id = result["category_id"]
        class_counts[cls_id] = class_counts.get(cls_id, 0) + 1
    print(f"  - ÌÅ¥ÎûòÏä§Î≥Ñ Í≤ÄÏ∂ú Ïàò: {class_counts}")
    
    # mAP Í≥ÑÏÇ∞
    try:
        coco_dt = coco_gt.loadRes(results)
        
        # IoU 0.5ÏóêÏÑúÏùò mAP
        print(f"\nüéØ ÏµúÏ†ÅÌôîÎêú ÌèâÍ∞Ä:")
        coco_eval_50 = COCOeval(coco_gt, coco_dt, "bbox")
        coco_eval_50.params.catIds = GOOD_CLASSES
        coco_eval_50.params.iouThrs = np.array([0.5])
        coco_eval_50.evaluate()
        coco_eval_50.accumulate()
        coco_eval_50.summarize()
        map_50 = coco_eval_50.stats[0]
        
        print(f"\n‚úÖ ÏµúÏ†ÅÌôîÎêú Í≤∞Í≥º:")
        print(f"  - mAP: {map_50:.3f}")
        
        return map_50
        
    except Exception as e:
        print(f"‚ùå mAP Í≥ÑÏÇ∞ Ï§ë Ïò§Î•ò: {e}")
        import traceback
        traceback.print_exc()
        return 0.0


# ===== Î©îÏù∏ Ïã§Ìñâ ÏΩîÎìú =====
if __name__ == "__main__":
    data_root = "./SFU/SFU_HW_Obj"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è  ÎîîÎ∞îÏù¥Ïä§: {device}")
    
    # üöÄ Í∞úÏÑ†Îêú Î™®Îç∏ ÏÉùÏÑ±
    print(f"\nüß† ÏµúÏ†ÅÌôîÎêú Î™®Îç∏ ÏÉùÏÑ±...")
    model = MyNetwork(
        num_classes=7,
        backbone_name='resnet50',
        min_size=800,
        max_size=1333,
        box_detections_per_img=300,  # Ï¶ùÍ∞Ä
        box_score_thresh=0.01,       # Í∞êÏÜå
        box_nms_thresh=0.3           # Í∞êÏÜå
    )
    model.to(device)
    
    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú
    checkpoint_path = "./model_checkpoint/best_model.pth"
    if not os.path.exists(checkpoint_path):
        checkpoint_files = [f for f in os.listdir("./model_checkpoint") if f.startswith("epoch_")]
        if checkpoint_files:
            checkpoint_files.sort(key=lambda x: int(x.split("_")[1].split(".")[0]))
            checkpoint_path = os.path.join("./model_checkpoint", checkpoint_files[-1])
        else:
            raise FileNotFoundError("Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§!")
    
    print(f"üìÅ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"], strict=True)
    model.eval()
    
    # Train.py ÌÅ¥ÎûòÏä§ Îß§Ìïë Ï†ïÎ≥¥ Ï∂îÏ∂ú
    train_cat2label = checkpoint.get("cat2label", None)
    train_label2cat = checkpoint.get("label2cat", None)
    
    if train_cat2label is None or train_label2cat is None:
        print(f"‚ùå Train.py ÌÅ¥ÎûòÏä§ Îß§Ìïë Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§!")
        exit(1)
    
    print(f"üè∑Ô∏è  Train.py ÌÅ¥ÎûòÏä§ Îß§Ìïë ÌôïÏù∏:")
    print(f"   - cat2label: {train_cat2label}")
    print(f"   - label2cat: {train_label2cat}")
    
    # Îç∞Ïù¥ÌÑ∞ÏÖã Î™©Î°ù ÌôïÏù∏
    if not os.path.exists(data_root):
        print(f"‚ùå SFU Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {data_root}")
        exit(1)
        
    datasets = sorted(os.listdir(data_root))
    print(f"\nüìÇ Î∞úÍ≤¨Îêú Îç∞Ïù¥ÌÑ∞ÏÖã: {len(datasets)}Í∞ú")
    
    results_dict = {}
    
    # Ïö∞ÏÑ†ÏàúÏúÑ Îç∞Ïù¥ÌÑ∞ÏÖãÎì§ Î®ºÏ†Ä ÌèâÍ∞Ä
    priority_datasets = [
        'PartyScene_832x480_50_val',
        'BasketballPass_416x240_50_val',
        'RaceHorses_416x240_30_val',
        'RaceHorses_832x480_30_val',
        'BQMall_832x480_60_val',
        'BQSquare_416x240_60_val',
        'BQTerrace_1920x1080_60_val',
        'Traffic_2560x1600_30_val',
        'ParkScene_1920x1080_24_val'
    ]
    
    for dataset_name in priority_datasets:
        if dataset_name not in datasets:
            continue
            
        print(f"\n{'='*60}")
        print(f"üöÄ {dataset_name} ÏµúÏ†ÅÌôîÎêú ÌèâÍ∞Ä")
        print(f"{'='*60}")
        
        try:
            dataset = SFUDatasetFixed(data_root, dataset_name, train_cat2label)
            test_dataloader = DataLoader(
                dataset, batch_size=1, shuffle=False, 
                num_workers=2, collate_fn=collate_fn
            )
            
            # ÏµúÏ†ÅÌôîÎêú ÌèâÍ∞Ä Ìï®Ïàò ÏÇ¨Ïö©
            map_50 = evaluate_mAP_optimized(
                model, test_dataloader, device, train_label2cat, dataset_name
            )
            
            results_dict[dataset_name] = {
                'mAP': map_50
            }
            
        except Exception as e:
            print(f"‚ùå {dataset_name} ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            import traceback
            traceback.print_exc()
            results_dict[dataset_name] = {'mAP': 0.0}
    
    # ÎÇòÎ®∏ÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖãÎì§ÎèÑ ÌèâÍ∞Ä
    for dataset_name in datasets:
        if dataset_name in priority_datasets:
            continue
            
        if not os.path.isdir(os.path.join(data_root, dataset_name)):
            continue
            
        print(f"\n{'='*60}")
        print(f"üìä {dataset_name} ÏµúÏ†ÅÌôîÎêú ÌèâÍ∞Ä")
        print(f"{'='*60}")
        
        try:
            dataset = SFUDatasetFixed(data_root, dataset_name, train_cat2label)
            test_dataloader = DataLoader(
                dataset, batch_size=1, shuffle=False, 
                num_workers=2, collate_fn=collate_fn
            )
            
            map_50 = evaluate_mAP_optimized(
                model, test_dataloader, device, train_label2cat, dataset_name
            )
            
            results_dict[dataset_name] = {
                'mAP': map_50
            }
            
        except Exception as e:
            print(f"‚ùå {dataset_name} ÌèâÍ∞Ä Ïã§Ìå®: {e}")
            results_dict[dataset_name] = {'mAP': 0.0}
    
    # ÏµúÏ¢Ö Í≤∞Í≥º Ï∂úÎ†•
    print(f"\n{'='*80}")
    print(f"üöÄ ÏµúÏ†ÅÌôîÎêú Test.py Í≤∞Í≥º")
    print(f"{'='*80}")
    
    print(f"{'Dataset':<30} {'mAP':<12} {'Status'}")
    print("-" * 80)
    
    total_map = 0
    count = 0
    
    for dataset_name, metrics in results_dict.items():
        map_50 = metrics['mAP']
        
        if map_50 > 0.4:
            status = "üöÄ Excellent"
        elif map_50 > 0.2:
            status = "‚úÖ Good"
        elif map_50 > 0.1:
            status = "‚ö†Ô∏è Fair"
        elif map_50 > 0.05:
            status = "üîÑ Low"
        elif map_50 > 0.0:
            status = "üîç Detected"
        else:
            status = "‚ùå Failed"
        
        print(f"{dataset_name:<30} {map_50:<12.3f} {status}")
        
        total_map += map_50
        count += 1
    
    